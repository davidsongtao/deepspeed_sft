{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:12:30.180628Z",
     "start_time": "2025-02-26T13:10:53.091060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 6601 /root/autodl-tmp/finetune.py \\\n",
    "    --model_name_or_path \"/root/autodl-fs/DeepSeek-R1-Distill-Qwen-32B\" \\\n",
    "    --data_path \"/root/autodl-tmp/data/datasets/output.json\" \\\n",
    "    --bf16 True \\\n",
    "    --output_dir \"/root/autodl-fs/trained_models/deepseek_ri_32b_sop\" \\\n",
    "    --num_train_epochs 50 \\\n",
    "    --per_device_train_batch_size 12 \\\n",
    "    --per_device_eval_batch_size 12 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --eval_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 902 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --adam_beta2 0.95 \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --report_to \"none\" \\\n",
    "    --model_max_length 512 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --deepspeed \"/root/autodl-tmp/deepspeed/ds_config_zero2.json\" \\\n",
    "    --use_lora\n"
   ],
   "id": "c6754e9b28a4867a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\r\n",
      "*****************************************\r\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "*****************************************\r\n",
      "[2025-02-26 21:10:57,038] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-02-26 21:10:57,041] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-02-26 21:10:57,041] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-02-26 21:10:57,042] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\r\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\r\n",
      "[2025-02-26 21:11:00,292] [INFO] [comm.py:658:init_distributed] cdb=None\r\n",
      "[2025-02-26 21:11:00,300] [INFO] [comm.py:658:init_distributed] cdb=None\r\n",
      "[2025-02-26 21:11:00,341] [INFO] [comm.py:658:init_distributed] cdb=None\r\n",
      "[2025-02-26 21:11:00,341] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n",
      "[2025-02-26 21:11:00,414] [INFO] [comm.py:658:init_distributed] cdb=None\r\n",
      "Loading checkpoint shards:  25%|████▌             | 2/8 [01:04<03:36, 36.11s/it]^C\r\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\r\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3840 closing signal SIGINT\r\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3841 closing signal SIGINT\r\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3842 closing signal SIGINT\r\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3843 closing signal SIGINT\r\n",
      "Loading checkpoint shards:  25%|████▌             | 2/8 [01:28<04:26, 44.41s/it]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 472, in <module>\r\n",
      "    train()\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 398, in train\r\n",
      "    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n",
      "    return model_class.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\r\n",
      "    ) = cls._load_pretrained_model(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4757, in _load_pretrained_model\r\n",
      "    gc.collect()\r\n",
      "KeyboardInterrupt\r\n",
      "Loading checkpoint shards:  25%|████▌             | 2/8 [01:28<04:26, 44.41s/it]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 472, in <module>\r\n",
      "Loading checkpoint shards:  25%|████▌             | 2/8 [01:28<04:26, 44.42s/it]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 472, in <module>\r\n",
      "    train()\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 398, in train\r\n",
      "    train()\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 398, in train\r\n",
      "    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n",
      "    return model_class.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\r\n",
      "    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n",
      "    return model_class.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\r\n",
      "    ) = cls._load_pretrained_model(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4757, in _load_pretrained_model\r\n",
      "    ) = cls._load_pretrained_model(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4757, in _load_pretrained_model\r\n",
      "    gc.collect()\r\n",
      "KeyboardInterrupt\r\n",
      "    gc.collect()\r\n",
      "KeyboardInterrupt\r\n",
      "Loading checkpoint shards:  25%|████▌             | 2/8 [01:28<04:26, 44.43s/it]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 472, in <module>\r\n",
      "    train()\r\n",
      "  File \"/root/autodl-tmp/finetune.py\", line 398, in train\r\n",
      "    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n",
      "    return model_class.from_pretrained(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\r\n",
      "    ) = cls._load_pretrained_model(\r\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 4757, in _load_pretrained_model\r\n",
      "    gc.collect()\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "35acf008-1dfe-4d32-8cf5-7022e042aadb",
   "metadata": {},
   "source": "## 权重融合"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61021499-4a44-45af-a682-943ed63c2fcb",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat/\", torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, \"output_qwen/\")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"output_qwen_merged\", max_shard_size=\"2048MB\", safe_serialization=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0dfbd261-6451-4532-82e8-3ae19ed93ee1",
   "metadata": {},
   "source": "## 分词器"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcba069-340b-4a93-a145-2028b425dd23",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat/\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"output_qwen_merged\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9f2878-79d3-4b1c-ba95-ac2f73aa6e1b",
   "metadata": {},
   "source": "## 模型测试"
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-26T11:49:14.690891Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torchvision\n",
    "\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-fs/DeepSeek-R1-Distill-Llama-70B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-fs/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "prompt = \"你好,你是谁？\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "response = tokenizer.decode(model.generate(**inputs, max_new_tokens=128000)[0], skip_special_tokens=True)\n",
    "print(response)"
   ],
   "id": "bc3f2a8586ad7424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7524e6628bd74c919ca694e01df27152"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 选一条样本对训练出来的模型进行人工比对",
   "id": "89992b6b55790e2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    \"conversations\": \n",
    "        [\n",
    "            {\n",
    "                \"from\": \"user\", \n",
    "                \"value\": \"你现在是一个很厉害的阅读理解器，严格按照人类指令进行回答。\\nInput: 下面描述中包含了哪些SPO？\\n返回json回答:\\n\\n花样男子花样男子《花样男子》，日文原名《花より男子》（Hana-yori Danshi），是日本漫画家神尾叶子的一部长篇爱情校园漫画，原作自1992年至2004年在集英社的漫画杂志《Margaret》上连载，并发行了37册的单行本，销量超过5900万册，霸占了日本第一畅销少女漫画的宝座。\"}, \n",
    "            {\n",
    "                \"from\": \"assistant\", \n",
    "                \"value\": \"[{\\\"predicate\\\": \\\"编剧\\\", \\\"object_type\\\": \\\"人物\\\", \\\"subject_type\\\": \\\"影视作品\\\", \\\"object\\\": \\\"神尾叶子\\\", \\\"subject\\\": \\\"花样男子\\\"}, {\\\"predicate\\\": \\\"出版社\\\", \\\"object_type\\\": \\\"出版社\\\", \\\"subject_type\\\": \\\"书籍\\\", \\\"object\\\": \\\"集英社\\\", \\\"subject\\\": \\\"花样男子\\\"}]\"\n",
    "            }\n",
    "        ], \n",
    "    \"id\": \"identity_0\"\n",
    "}"
   ],
   "id": "6aceadf2c3883c9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
